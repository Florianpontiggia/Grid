{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "<img src=\"./logo.jpg\", width=150, ALIGN=\"left\", border=20>\n",
    "<h1>L2RPN Starting Kit </h1> \n",
    "\n",
    "<br>This code was tested with <br>\n",
    "Python 3.6.6 |Anaconda custom (64-bit)| (default, Nov 2018, 11:07:29) (https://anaconda.org/)<br>\n",
    "<i> Adapted for Chalab by Isabelle Guyon from original code of Balázs Kégl</i> <br>\n",
    "<a href=\"http://www.datascience-paris-saclay.fr\">Paris Saclay Center for Data Science (CDS)</a>\n",
    "</center>\n",
    "<p>\n",
    "ALL INFORMATION, SOFTWARE, DOCUMENTATION, AND DATA ARE PROVIDED \"AS-IS\". The CDS, CHALEARN, AND/OR OTHER ORGANIZERS OR CODE AUTHORS DISCLAIM ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR ANY PARTICULAR PURPOSE, AND THE WARRANTY OF NON-INFRIGEMENT OF ANY THIRD PARTY'S INTELLECTUAL PROPERTY RIGHTS. IN NO EVENT SHALL AUTHORS AND ORGANIZERS BE LIABLE FOR ANY SPECIAL, \n",
    "INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF SOFTWARE, DOCUMENTS, MATERIALS, PUBLICATIONS, OR INFORMATION MADE AVAILABLE FOR THE CHALLENGE. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h2>Introduction </h2>\n",
    "    <p> \n",
    "     <br>\n",
    "The goal of this challenge is to use Reinforcement Learning in Power Grid management by designing RL agents to automate the control of the power grid. The dataset used in this challenge is from <a href=\"https://github.com/MarvinLer/pypownet\">pypownet</a>, made by Marvin Lerousseau, it is a simulator that is able to emulate a power grid of any size and electrical properties subject to a set of temporal injections for discretized time-steps.\n",
    "\n",
    "References and credits: <br>\n",
    "Founder of pypownet was Marvin Lerousseau. The competition protocol was designed by Isabelle Guyon. Our mentors are Balthazar Donon and Antoine Marot. Pypownet, 2017. https://github.com/MarvinLer/pypownet. The baseline methods were inspired by work performed by Kimang Khun.\n",
    " <br> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'example_submission/'\n",
    "problem_dir = 'ingestion_program/'  \n",
    "score_dir = 'scoring_program/'\n",
    "input_dir = 'public_data/'\n",
    "output_dir = 'output/'\n",
    "from sys import path; path.append(model_dir); path.append(problem_dir); path.append(score_dir);\n",
    "path.append(input_dir); path.append(output_dir);\n",
    "%matplotlib inline\n",
    "# Uncomment the next lines to auto-reload libraries (this causes some problem with pickles in Python 3)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import seaborn as sns; sns.set()\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background:#FFFFAA\">\n",
    "    <h1> Step 1: Exploratory data analysis </h1>\n",
    "<p>\n",
    "We provide data with the starting kit.\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electrical grid\n",
    "<div >\n",
    "<img src=\"./ExampleGrid.JPG\", width=750, ALIGN=\"left\", border=20>\n",
    "    <br>\n",
    "    <br>\n",
    "(courtesy of Marvin Lerousseau)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the challenge, a grid of 14 substations is given. 20 lines connected the nodes of the network.\n",
    "\n",
    "For the following example, we take the case where there are 11 loads and 5 prods and particularly the hard level. Furthermore, the information shown are only those of January."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chronics  configuration.yaml  reference_grid.m\r\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'public_data/hard'              # Change this to the directory where you put the input data\n",
    "!ls $data_dir*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we load the data as a \"pandas\" data frame, so we can use \"pandas\" to explore the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "<h1>Step 2: Building an Agent</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h2>Loading data with pypownet</h2>\n",
    "    <p>\n",
    "We reload the data with the environment class of pypownet\n",
    "   <br>\n",
    "    \n",
    "To win, flows in a line have to stay under a threshold. Above this threshold, the line will overheat and after a certain amount of overheating, the line will break. Thermal limits are already defined in pypownet.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom reward signal CustomRewardSignal of file /home/nicolas/test/Grid2/starting_kit/public_data/reward_signal.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ====================================================\n",
      "                     GAME PARAMETERS\n",
      "  ====================================================\n",
      "    hard_overflow_coefficient: 1.0\n",
      "    loadflow_backend: pypower\n",
      "    loadflow_mode: AC\n",
      "    max_number_loads_game_over: 6\n",
      "    max_number_prods_game_over: 3\n",
      "    max_seconds_per_timestep: 1.0\n",
      "    n_timesteps_consecutive_soft_overflow_breaks: 10\n",
      "    n_timesteps_hard_overflow_is_broken: 10\n",
      "    n_timesteps_horizon_maintenance: 48\n",
      "    n_timesteps_soft_overflow_is_broken: 10\n",
      "  ====================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pypownet.environment\n",
    "import pypownet.runner\n",
    "data_dir = 'public_data'  \n",
    "environment = pypownet.environment.RunEnv(parameters_folder=os.path.abspath(data_dir),\n",
    "                                              game_level=\"hard\",\n",
    "                                              chronic_looping_mode='natural', start_id=0,\n",
    "                                              game_over_mode=\"soft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h2>Building an agent</h2>\n",
    "    <p>\n",
    "We provide examples of agent (for reinforcement learning) in the `starting-kit/example_submission` directory. It is a quite stupid agent: it does nothing. Replace it with your own agent.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring the results of an agent\n",
    "\n",
    "<div style=\"background:#FFFFAA\">\n",
    "    <br>\n",
    "    <p>\n",
    "<b>The metric chosen for your challenge</b> is identified in the \"metric.txt\" file found in the `scoring_function/` directory. The function \"get_metric\" searches first for a metric having that name in my_metric.py, then in libscores.py, then in sklearn.metric.\n",
    "    <br>\n",
    "The aim of a reinforcement learning problem is to maximize the reward function.\n",
    "\n",
    "When running the agent, two values are given back : the first one is the reward of the last timestep and the second one is the cumulative reward for all the iterations of the run of the agent. The reward indicates if the game is going towards a game over or not.\n",
    "\n",
    "Specifically, our reward function is composed of 5 subrewards. They describe the proportion of isolated productions, loads, the cost of an action, an indication of the amount of changes between the current grid and the initial grid and lastly information on the lines capacity usage. \n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using scoring metric: reward\n"
     ]
    }
   ],
   "source": [
    "from scoring_program import libscores\n",
    "from libscores import get_metric\n",
    "metric_name, scoring_function = get_metric()\n",
    "print('Using scoring metric:', metric_name)\n",
    "# Uncomment the next line to display the code of the scoring metric\n",
    "#??scoring_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0265579223632812e-05\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAgent(pypownet.agent.Agent):\n",
    "    \"\"\"\n",
    "    An example of a baseline controler that randomly switches the status of one random power line per timestep (if the\n",
    "    random line is previously online, switch it off, otherwise switch it on).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, environment):\n",
    "        super().__init__(environment)\n",
    "        self.verbose = True\n",
    "\n",
    "    def act(self, observation):\n",
    "        # Sanity check: an observation is a structured object defined in the environment file.\n",
    "        assert isinstance(observation, pypownet.environment.Observation)\n",
    "        action_space = self.environment.action_space\n",
    "\n",
    "        # Create template of action with no switch activated (do-nothing action)\n",
    "        action = action_space.get_do_nothing_action()\n",
    "\n",
    "        # Select lines to switch\n",
    "        if True :\n",
    "            lines_load = observation.get_lines_capacity_usage()\n",
    "            nb_lines = len(lines_load)\n",
    "            assert nb_lines == action_space.lines_status_subaction_length\n",
    "            for i in range(nb_lines):\n",
    "                lines_status = action_space.get_lines_status_switch_from_id(action,i)\n",
    "                if lines_status == 0:\n",
    "                    action_space.set_lines_status_switch_from_id(action=action,line_id=i,new_switch_value=0)\n",
    "                if lines_load[i] > 1:\n",
    "                    action_space.set_lines_status_switch_from_id(action=action,line_id=i,new_switch_value=1)\n",
    "                    action_name = 'switching status of line %d' % i\n",
    "                    if self.verbose:\n",
    "                        print('Action chosen: ', action_name, '; expected reward %.4f' % reward)\n",
    "\n",
    "\n",
    "        # Test the reward on the environment\n",
    "        reward_aslist = self.environment.simulate(action, do_sum=False)\n",
    "        reward = sum(reward_aslist)\n",
    "        if self.verbose:\n",
    "            print('reward: [', ', '.join(['%.2f' % c for c in reward_aslist]), '] =', reward)\n",
    "\n",
    "\n",
    "        return action\n",
    "\n",
    "        # No learning (i.e. self.feed_reward does pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.25 ] = -0.24796949553161035\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.23 ] = -0.2336597951442309\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.22 ] = -0.21913744390058026\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.20 ] = -0.20384386816027017\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.19 ] = -0.19069972264196305\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.18 ] = -0.17744492936380044\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.16 ] = -0.16367978698921049\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.15 ] = -0.152185684456322\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.14 ] = -0.14216176841593572\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.13 ] = -0.1349605806565615\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.13 ] = -0.1288497684212152\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.12 ] = -0.12406685688698504\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.12 ] = -0.12088946521997883\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.12 ] = -0.11828113986188324\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.12 ] = -0.11585946832866106\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.11 ] = -0.11351546519813029\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.11 ] = -0.11118384483163843\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.11 ] = -0.10851570628794707\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.11 ] = -0.10625118743480358\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.10 ] = -0.10398305148799561\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.10 ] = -0.10190093920067675\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.10 ] = -0.10006325038734894\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.10 ] = -0.09924635106467156\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.10 ] = -0.09795334274399231\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.10 ] = -0.09735166860395894\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.10 ] = -0.09704341983854879\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.10 ] = -0.09645070113340752\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.10 ] = -0.09540477377934083\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.09 ] = -0.09486062677231152\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.09 ] = -0.0937122156372685\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.09 ] = -0.09249714728549766\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.09 ] = -0.09236283099043911\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.09 ] = -0.09222378780911455\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.09 ] = -0.09226805681392396\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.09 ] = -0.09249349331210849\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.09 ] = -0.09301608530533947\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.09 ] = -0.09382702085683609\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.09 ] = -0.094949132472228\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.10 ] = -0.0960103693680373\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.10 ] = -0.09700127410299537\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.10 ] = -0.09849106886516606\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.10 ] = -0.09981368871938692\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.10 ] = -0.10167350894786618\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.10 ] = -0.10353920128659666\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.10 ] = -0.10445489664801474\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.10 ] = -0.10471425338754047\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.10 ] = -0.10471425338754047\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.10 ] = -0.10467426376986726\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.10 ] = -0.10471425338754047\n",
      "reward: [ -0.00, -0.00, 0.00, -0.00, -0.10 ] = -0.10451253016187978\n",
      "cumulative rewards : -11.269334237132293\n",
      "1.101823329925537\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import time\n",
    "start = time.time()\n",
    "NUMBER_ITERATIONS = 50\n",
    "\n",
    "submission_dir = 'example_submission'\n",
    "sys.path.append(submission_dir)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "log_path = os.path.abspath(os.path.join(output_dir, 'runner.log'))\n",
    "\n",
    "\n",
    "open(log_path, 'w').close()\n",
    "submitted_controler = CustomAgent(environment)\n",
    "# Instanciate a runner, that will save the run statistics within the log_path file, to be parsed and processed\n",
    "# by the scoring program\n",
    "phase_runner = pypownet.runner.Runner(environment, submitted_controler, verbose=True, vverbose=False,\n",
    "                                      log_filepath=log_path)\n",
    "phase_runner.ch.setLevel(logging.ERROR)\n",
    "# Run the planned experiment of this phase with the submitted model\n",
    "score = phase_runner.loop(iterations=NUMBER_ITERATIONS)\n",
    "print(\"cumulative rewards : {}\".format(score))\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <b> Save the best agent </b> it should be a class Submission and save in \"example_submission/submission.py\".  Uncomment the line <i>%%writefile example_submission/submission.py to save the agent. </i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile example_submission/submission.py\n",
    "import pypownet.agent\n",
    "import pypownet.environment\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class Submission(pypownet.agent.Agent):\n",
    "    \"\"\"\n",
    "    An example of a baseline controler that randomly switches the status of one random power line per timestep (if the\n",
    "    random line is previously online, switch it off, otherwise switch it on).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, environment):\n",
    "        super().__init__(environment)\n",
    "        self.verbose = True\n",
    "\n",
    "    def act(self, observation):\n",
    "        # Sanity check: an observation is a structured object defined in the environment file.\n",
    "        assert isinstance(observation, pypownet.environment.Observation)\n",
    "        action_space = self.environment.action_space\n",
    "\n",
    "        # Create template of action with no switch activated (do-nothing action)\n",
    "        action = action_space.get_do_nothing_action()\n",
    "\n",
    "        # Select lines to switch\n",
    "        if True :\n",
    "            lines_load = observation.get_lines_capacity_usage()\n",
    "            nb_lines = len(lines_load)\n",
    "            assert nb_lines == action_space.lines_status_subaction_length\n",
    "            for i in range(nb_lines):\n",
    "                lines_status = action_space.get_lines_status_switch_from_id(action,i)\n",
    "                if lines_status == 0:\n",
    "                    action_space.set_lines_status_switch_from_id(action=action,line_id=i,new_switch_value=0)\n",
    "                if lines_load[i] > 1:\n",
    "                    action_space.set_lines_status_switch_from_id(action=action,line_id=i,new_switch_value=1)\n",
    "                    action_name = 'switching status of line %d' % i\n",
    "                    if self.verbose:\n",
    "                        print('Action chosen: ', action_name, '; expected reward %.4f' % reward)\n",
    "\n",
    "\n",
    "        # Test the reward on the environment\n",
    "        reward_aslist = self.environment.simulate(action, do_sum=False)\n",
    "        reward = sum(reward_aslist)\n",
    "        if self.verbose:\n",
    "            print('reward: [', ', '.join(['%.2f' % c for c in reward_aslist]), '] =', reward)\n",
    "\n",
    "\n",
    "        return action\n",
    "\n",
    "        # No learning (i.e. self.feed_reward does pass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background:#FFFFAA\">\n",
    "<h1> Step 3: Making a submission </h1> \n",
    "\n",
    "<h2> Unit testing </h2> \n",
    "\n",
    "It is <b><span style=\"color:red\">important that you test your submission files before submitting them</span></b>. All you have to do to make a submission is modify the file <code>submission.py</code> in the <code>starting_kit/example_submission/</code> directory, then run this test to make sure everything works fine. This is the actual program that will be run on the server to test your submission. \n",
    "<br>\n",
    "Keep the sample code simple.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dir: /home/nicolas/test/Grid2/starting_kit/public_data\n",
      "output dir: /home/nicolas/test/Grid2/starting_kit/public_data/res\n",
      "program dir: /home/nicolas/test/Grid2/starting_kit/ingestion_program\n",
      "submission dir: /home/nicolas/test/Grid2/starting_kit/example_submission\n",
      "input content ['medium', '__pycache__', 'reward_signal.py', 'hard', 'level0', 'easy', 'res']\n",
      "output content ['runner.log']\n",
      "program content ['data_io.py', '__pycache__', 'data_manager.py', 'data_converter.py', 'ingestion.py', 'metadata']\n",
      "submission content ['my_agents.py', '__pycache__', 'submission.py', 'baseline_agents.py', '#my_agents.py#', 'metadata']\n",
      "Using custom reward signal CustomRewardSignal of file /home/nicolas/test/Grid2/starting_kit/public_data/reward_signal.py\n",
      "\n",
      "  ====================================================\n",
      "                     GAME PARAMETERS\n",
      "  ====================================================\n",
      "    hard_overflow_coefficient: 1.0\n",
      "    loadflow_backend: pypower\n",
      "    loadflow_mode: AC\n",
      "    max_number_loads_game_over: 6\n",
      "    max_number_prods_game_over: 3\n",
      "    max_seconds_per_timestep: 1.0\n",
      "    n_timesteps_consecutive_soft_overflow_breaks: 10\n",
      "    n_timesteps_hard_overflow_is_broken: 10\n",
      "    n_timesteps_horizon_maintenance: 48\n",
      "    n_timesteps_soft_overflow_is_broken: 10\n",
      "  ====================================================\n",
      "\n",
      "log file path /home/nicolas/test/Grid2/starting_kit/public_data/res/runner.log\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.33 ] = -0.12941295187642377\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.32 ] = -0.11632361399380392\n",
      "/home/nicolas/anaconda3/envs/rte2/lib/python3.6/site-packages/scipy/sparse/linalg/dsolve/linsolve.py:193: MatrixRankWarning: Matrix is exactly singular\n",
      "  warn(\"Matrix is exactly singular\", MatrixRankWarning)\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.31 ] = -0.10503946696910876\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.29 ] = -0.0940177464875972\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.28 ] = -0.08159431741844747\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.27 ] = -0.06993116640897695\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.26 ] = -0.0572275294739914\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.25 ] = -0.046869993551317546\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.24 ] = -0.036477395031610765\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.23 ] = -0.031003622437430434\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.22 ] = -0.024511978022540765\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.22 ] = -0.02080105020860068\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.22 ] = -0.019084869417560824\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.22 ] = -0.018462634694799\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.22 ] = -0.016304215915210557\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.21 ] = -0.014776949973947573\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.21 ] = -0.013108101635891212\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.21 ] = -0.010661420903441937\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.21 ] = -0.008349903129165781\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.21 ] = -0.00514019854538697\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.20 ] = -0.0023861929775662594\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.20 ] = -0.0018639308387002063\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.20 ] = 1.279967587372588e-05\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.20 ] = 0.0002312940190451751\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.20 ] = 0.0009536554496711314\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.20 ] = 0.0015347176171149612\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.20 ] = 0.002100568340021425\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.20 ] = 0.004263768477306001\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.19 ] = 0.008072487610037704\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.19 ] = 0.009988163057797927\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.18 ] = 0.015103203941070537\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.18 ] = 0.019928500270396304\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.18 ] = 0.02291420549214923\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.18 ] = 0.02471512088566946\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.17 ] = 0.027304707260195488\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.17 ] = 0.029271680466967237\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.17 ] = 0.03047686533806629\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.17 ] = 0.03082533645868643\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.17 ] = 0.030744719815964328\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.17 ] = 0.032014029773435315\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.17 ] = 0.03220404838631019\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.17 ] = 0.03324165879399149\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.17 ] = 0.0346906966230473\n",
      "reward: [ -0.00, -0.00, 0.20, -0.00, -0.17 ] = 0.03494353200565431\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"ingestion_program//ingestion.py\", line 82, in <module>\n",
      "    main()\n",
      "  File \"ingestion_program//ingestion.py\", line 74, in main\n",
      "    phase_runner.loop(iterations=NUMBER_ITERATIONS)\n",
      "  File \"/home/nicolas/anaconda3/envs/rte2/lib/python3.6/site-packages/pypownet-2.0.4-py3.6.egg/pypownet/runner.py\", line 109, in loop\n",
      "    (obs, act, rew) = self.step()\n",
      "  File \"/home/nicolas/anaconda3/envs/rte2/lib/python3.6/site-packages/pypownet-2.0.4-py3.6.egg/pypownet/runner.py\", line 79, in step\n",
      "    action = self.agent.act(self.last_observation)\n",
      "  File \"/home/nicolas/test/Grid2/starting_kit/example_submission/submission.py\", line 45, in act\n",
      "    newBestAction = self.chooseAction(bestAction,rew)\n",
      "  File \"/home/nicolas/test/Grid2/starting_kit/example_submission/submission.py\", line 27, in chooseAction\n",
      "    rewards.append(sum(self.environment.simulate(act, do_sum = False)))\n",
      "  File \"/home/nicolas/anaconda3/envs/rte2/lib/python3.6/site-packages/pypownet-2.0.4-py3.6.egg/pypownet/environment.py\", line 611, in simulate\n",
      "    observation, reward_flag, done = self.game.simulate(to_simulate_action)\n",
      "  File \"/home/nicolas/anaconda3/envs/rte2/lib/python3.6/site-packages/pypownet-2.0.4-py3.6.egg/pypownet/game.py\", line 606, in simulate\n",
      "    simulated_obs, flag, done = self.step(action, _is_simulation=True)\n",
      "  File \"/home/nicolas/anaconda3/envs/rte2/lib/python3.6/site-packages/pypownet-2.0.4-py3.6.egg/pypownet/game.py\", line 556, in step\n",
      "    self._compute_loadflow_cascading()\n",
      "  File \"/home/nicolas/anaconda3/envs/rte2/lib/python3.6/site-packages/pypownet-2.0.4-py3.6.egg/pypownet/game.py\", line 370, in _compute_loadflow_cascading\n",
      "    self.grid.compute_loadflow(fname_end='_cascading%d' % depth)\n",
      "  File \"/home/nicolas/anaconda3/envs/rte2/lib/python3.6/site-packages/pypownet-2.0.4-py3.6.egg/pypownet/grid.py\", line 243, in compute_loadflow\n",
      "    output, loadflow_success = self.__vanilla_loadflow_backend_callback(fname_end=fname_end)\n",
      "  File \"/home/nicolas/anaconda3/envs/rte2/lib/python3.6/site-packages/pypownet-2.0.4-py3.6.egg/pypownet/grid.py\", line 217, in __vanilla_loadflow_backend_callback\n",
      "    output, loadflow_success = function(self.mpc, self.loadflow_options, pprint, fname)\n",
      "  File \"/home/nicolas/anaconda3/envs/rte2/lib/python3.6/site-packages/PYPOWER-5.1.4-py3.6.egg/pypower/rundcpf.py\", line 26, in rundcpf\n",
      "    return runpf(casedata, ppopt, fname, solvedcase)\n",
      "  File \"/home/nicolas/anaconda3/envs/rte2/lib/python3.6/site-packages/PYPOWER-5.1.4-py3.6.egg/pypower/runpf.py\", line 285, in runpf\n",
      "    results = int2ext(ppc)\n",
      "  File \"/home/nicolas/anaconda3/envs/rte2/lib/python3.6/site-packages/PYPOWER-5.1.4-py3.6.egg/pypower/int2ext.py\", line 43, in int2ext\n",
      "    ppc = deepcopy(ppc)\n",
      "  File \"/home/nicolas/anaconda3/envs/rte2/lib/python3.6/copy.py\", line 180, in deepcopy\n",
      "    y = _reconstruct(x, memo, *rv)\n",
      "  File \"/home/nicolas/anaconda3/envs/rte2/lib/python3.6/copy.py\", line 306, in _reconstruct\n",
      "    value = deepcopy(value, memo)\n",
      "  File \"/home/nicolas/anaconda3/envs/rte2/lib/python3.6/copy.py\", line 161, in deepcopy\n",
      "    y = copier(memo)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python $problem_dir/ingestion.py $input_dir $input_dir/res $problem_dir $model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background:#FFFFAA\">\n",
    "Also test the scoring program:\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public_data/\r\n",
      "output\r\n",
      "step : 44, cumulative rewards : -2.7\r\n"
     ]
    }
   ],
   "source": [
    "scoring_output_dir = 'output'\n",
    "!python $score_dir/evaluate.py $input_dir $scoring_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h1> Preparing the submission </h1>\n",
    "\n",
    "Zip the contents of `sample_code_submission/` (without the directory), or download the challenge public_data and run the command in the previous cell, after replacing sample_data by public_data.\n",
    "Then zip the contents of `sample_result_submission/` (without the directory).\n",
    "<b><span style=\"color:red\">Do NOT zip the data with your submissions</span></b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit one of these files:\n",
      "sample_code_submission_19-03-21-17-41.zip\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "from data_io import zipdir\n",
    "the_date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")\n",
    "sample_code_submission = 'sample_code_submission_' + the_date + '.zip' \n",
    "zipdir(sample_code_submission, model_dir) \n",
    "print(\"Submit one of these files:\\n\" + sample_code_submission + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
